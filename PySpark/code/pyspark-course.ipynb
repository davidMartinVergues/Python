{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INDEX\n",
    "\n",
    "[Creamos sesion de Spark](###Creamos-sesion-de-Spark)  \n",
    "[Accediendo a los valores del dataframe](###Accediendo-a-los-valores-del-dataframe)  \n",
    "[Creamos sesion de Spark](###Creamos-sesion-de-Spark)  \n",
    "[Creamos sesion de Spark](###Creamos-sesion-de-Spark)  \n",
    "[Creamos sesion de Spark](###Creamos-sesion-de-Spark)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creamos sesion de Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. importamos sparksession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.5\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845498 sha256=245b98c1dda982e95c219ce0e9e576ed490344a4dd67353e4fb54bb946fd3a5b\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/9c/aa/b1/8433fd8b1afe7eb31196cc74a42cd778bcb52636a428da079d\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Con el constructor le creamos la session y le damos un nombre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('pyspark-course').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Imprimimos los datos de la nueva sesión creada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://31e019bb603e:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-course</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4e15ab1840>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. AHora podemos leer un dataset contenido en un archivo CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_education_level = spark.read.option('header',True).csv('csv/credit_card_education_level.csv')\n",
    "credit_card_info = spark.read.option('header',True).csv('csv/credit_card_info.csv')\n",
    "test1 = spark.read.option('header',True).csv('csv/test1.csv')\n",
    "test2 = spark.read.option('header',True).csv('csv/test3.csv')\n",
    "test2 = spark.read.option('header',True).csv('csv/tips.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+--------+\n",
      "|     Name|age|Experience|  Salary|\n",
      "+---------+---+----------+--------+\n",
      "|    Krish| 31|        10|   30000|\n",
      "|Sudhanshu| 30|         8|25000.55|\n",
      "|    Sunny| 29|         4|   20000|\n",
      "|     Paul| 24|         3|   20000|\n",
      "|   Harsha| 21|         1|15000.98|\n",
      "|  Shubham| 23|         2|18000.99|\n",
      "+---------+---+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accediendo a los valores del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Name='Krish', age=31, Experience=10, Salary=30000.0)\n",
      "[Row(Name='Krish', age=31, Experience=10, Salary=30000.0), Row(Name='Sudhanshu', age=30, Experience=8, Salary=25000.55), Row(Name='Sunny', age=29, Experience=4, Salary=20000.0)]\n"
     ]
    }
   ],
   "source": [
    "# podemos mostrar tb las primeras filas del dataframe\n",
    "\n",
    "\n",
    "print(test1.head())  # devuelve la primera fila\n",
    "print(test1.head(3)) # devuelve una lista de filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Krish 31 10 30000.0\n",
      "[Row(Name='Krish', age=31, Experience=10, Salary=30000.0), Row(Name='Sudhanshu', age=30, Experience=8, Salary=25000.55)]\n"
     ]
    }
   ],
   "source": [
    "name,age,experience,salary = test1.head()           # DESTRUCTURING\n",
    "my_data_list = [*test1.head(2)]                      # SPREAD OPERATOR\n",
    "\n",
    "print(name,age,experience,salary)\n",
    "print(my_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- Experience: string (nullable = true)\n",
      " |-- Salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vamos a ver el esquema del dataframe\n",
    "\n",
    "test1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por defecto cuando leemos un archivo pyspark coge los datos como strings, si queremos que automáticamente pyspark infiera el tipo de datos debemos usar la opción `inferSchema=True` en el read csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = spark.read.option('header',True).csv('csv/test1.csv',inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+--------+\n",
      "|     Name|age|Experience|  Salary|\n",
      "+---------+---+----------+--------+\n",
      "|    Krish| 31|        10| 30000.0|\n",
      "|Sudhanshu| 30|         8|25000.55|\n",
      "|    Sunny| 29|         4| 20000.0|\n",
      "|     Paul| 24|         3| 20000.0|\n",
      "|   Harsha| 21|         1|15000.98|\n",
      "|  Shubham| 23|         2|18000.99|\n",
      "+---------+---+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ahora tanto edad experiencia y salario los interpreta como integers\n",
    "test1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+--------+\n",
      "|     Name|age|Experience|  Salary|\n",
      "+---------+---+----------+--------+\n",
      "|    Krish| 31|        10| 30000.0|\n",
      "|Sudhanshu| 30|         8|25000.55|\n",
      "|    Sunny| 29|         4| 20000.0|\n",
      "|     Paul| 24|         3| 20000.0|\n",
      "|   Harsha| 21|         1|15000.98|\n",
      "|  Shubham| 23|         2|18000.99|\n",
      "+---------+---+----------+--------+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PODEMOS HACER TODOS ESTOS PASOS EN UN ÚNICO PASO\n",
    "\n",
    "test1_v2 = spark.read.csv('csv/test1.csv', header=True,inferSchema=True)\n",
    "test1_v2.show()\n",
    "test1_v2.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si miramos el tipo de este dato, test1 / test1_v2, son ambos **dataframe**. Un dataframe es una estructura de datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(test1))\n",
    "print(type(test1_v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Selecting column and indexing\n",
    "\n",
    "En primer lugar tenemos el atributo **columns** del datframe que nos devuelve una lista con el nombre de las columnas `df.columns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'age', 'Experience', 'Salary']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1_v2.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver los datos que contiene una columna usamos el método `select()` esto nos devolverá otro dataframe pero solo con la columna especificada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+---------+\n",
      "|     name|\n",
      "+---------+\n",
      "|    Krish|\n",
      "|Sudhanshu|\n",
      "|    Sunny|\n",
      "|     Paul|\n",
      "|   Harsha|\n",
      "|  Shubham|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "colummna_name = test1_v2.select('name')\n",
    "print(type(colummna_name))\n",
    "colummna_name.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos recoger múltiples columnas pasamos los nombres en una lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|     name|age|\n",
      "+---------+---+\n",
      "|    Krish| 31|\n",
      "|Sudhanshu| 30|\n",
      "|    Sunny| 29|\n",
      "|     Paul| 24|\n",
      "|   Harsha| 21|\n",
      "|  Shubham| 23|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columnas_name_age = test1_v2.select(['name','age'])\n",
    "columnas_name_age.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Krish'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recoger los datos de los registros.\n",
    "\n",
    "columnas_name_age.collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos coger los valores de la columna indicando directamente el nombre sobre el DF pero esto nos devolverá un tipo `column` en lugar de DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.column.Column'>\n"
     ]
    }
   ],
   "source": [
    "column_name = test1_v2['name']\n",
    "\n",
    "print(type(column_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos sacar los datatypes de las distintas columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'),\n",
       " ('age', 'int'),\n",
       " ('Experience', 'int'),\n",
       " ('Salary', 'double')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1_v2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos disponible la opción `describe`, este método nos devuelve un dataframe con información sobre los distintos campos, si son numéricos nos da info sobre la media, SD, valores min y max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------------+-----------------+------------------+\n",
      "|summary|  Name|               age|       Experience|            Salary|\n",
      "+-------+------+------------------+-----------------+------------------+\n",
      "|  count|     6|                 6|                6|                 6|\n",
      "|   mean|  null|26.333333333333332|4.666666666666667|21333.753333333334|\n",
      "| stddev|  null| 4.179314138308661|3.559026084010437| 5353.846365332748|\n",
      "|    min|Harsha|                21|                1|          15000.98|\n",
      "|    max| Sunny|                31|               10|           30000.0|\n",
      "+-------+------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test1_v2.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding / dropping columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding\n",
    "\n",
    "Para que esta operación quede grabada debemos asignarle el resultado a una nueva variable o reasignarle el resultado a la variable ya creada. \n",
    "Debe recibir como parámetro:\n",
    "1. el nombre de la columna (si no existe dicha columna se crea si ya existe se sobreescribe)\n",
    "2. los datos (una instancia de la clase column)\n",
    "\n",
    "Para acceder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+--------+------------------------+\n",
      "|     Name|age|Experience|  Salary|Experience after 2 years|\n",
      "+---------+---+----------+--------+------------------------+\n",
      "|    Krish| 31|        10| 30000.0|                      12|\n",
      "|Sudhanshu| 30|         8|25000.55|                      10|\n",
      "|    Sunny| 29|         4| 20000.0|                       6|\n",
      "|     Paul| 24|         3| 20000.0|                       5|\n",
      "|   Harsha| 21|         1|15000.98|                       3|\n",
      "|  Shubham| 23|         2|18000.99|                       4|\n",
      "+---------+---+----------+--------+------------------------+\n",
      "\n",
      "+---------+---+----------+--------+------------------------+\n",
      "|     Name|age|Experience|  Salary|Experience after 2 years|\n",
      "+---------+---+----------+--------+------------------------+\n",
      "|    Krish| 31|        10| 30000.0|                      12|\n",
      "|Sudhanshu| 30|         8|25000.55|                      10|\n",
      "|    Sunny| 29|         4| 20000.0|                       6|\n",
      "|     Paul| 24|         3| 20000.0|                       5|\n",
      "|   Harsha| 21|         1|15000.98|                       3|\n",
      "|  Shubham| 23|         2|18000.99|                       4|\n",
      "+---------+---+----------+--------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = test1_v2.withColumn(\"Experience after 2 years\",test1_v2.Experience+2)\n",
    "test1_v2.withColumn(\"Experience after 2 years\",test1_v2.select('experience')[0]+2).show()\n",
    "test1_v2.withColumn(\"Experience after 2 years\",test1_v2['Experience']+2).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop column\n",
    "\n",
    "Solo necesitamos pasar el nombre de la columna a eliminar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+--------+------------------------+\n",
      "|     Name|age|Experience|  Salary|Experience after 2 years|\n",
      "+---------+---+----------+--------+------------------------+\n",
      "|    Krish| 31|        10| 30000.0|                      12|\n",
      "|Sudhanshu| 30|         8|25000.55|                      10|\n",
      "|    Sunny| 29|         4| 20000.0|                       6|\n",
      "|     Paul| 24|         3| 20000.0|                       5|\n",
      "|   Harsha| 21|         1|15000.98|                       3|\n",
      "|  Shubham| 23|         2|18000.99|                       4|\n",
      "+---------+---+----------+--------+------------------------+\n",
      "\n",
      "+---------+---+----------+--------+\n",
      "|     Name|age|Experience|  Salary|\n",
      "+---------+---+----------+--------+\n",
      "|    Krish| 31|        10| 30000.0|\n",
      "|Sudhanshu| 30|         8|25000.55|\n",
      "|    Sunny| 29|         4| 20000.0|\n",
      "|     Paul| 24|         3| 20000.0|\n",
      "|   Harsha| 21|         1|15000.98|\n",
      "|  Shubham| 23|         2|18000.99|\n",
      "+---------+---+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test.show()\n",
    "test = test.drop('Experience after 2 years')\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rename Columns\n",
    "\n",
    "usando la función `withColumnRenamed()`\n",
    "\n",
    "Solo debemos pasar el nombre de la columna y el nuevo nombre  q adoptará"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+----------+--------+\n",
      "|     Name|new age column|Experience|  Salary|\n",
      "+---------+--------------+----------+--------+\n",
      "|    Krish|            31|        10| 30000.0|\n",
      "|Sudhanshu|            30|         8|25000.55|\n",
      "|    Sunny|            29|         4| 20000.0|\n",
      "|     Paul|            24|         3| 20000.0|\n",
      "|   Harsha|            21|         1|15000.98|\n",
      "|  Shubham|            23|         2|18000.99|\n",
      "+---------+--------------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = test.withColumnRenamed('age','new age column')\n",
    "\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|      null|  null|\n",
      "|     null|null|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test2 = spark.read.csv('csv/test2.csv', header=True,inferSchema=True)\n",
    "test2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deleting missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos elimnar estos registros con valores missing/null utilizando `na` y la función `drop()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test2.na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la función drop le podemos pasar 3 parámetros:\n",
    "1. how\n",
    "2. subset\n",
    "3. thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How* puede aceptar dos valores \n",
    "\n",
    "1. all =>  elimina ese registro solo cuando todos los campos de un registro son null\n",
    "2. any => elimina ese registro si alguno de sus campos es null (valor por defecto)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n",
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test2.na.drop(how='all').show()\n",
    "test2.na.drop(how='any').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*thresh* puedes marcar cuantos valores informados como mínimo puede tener un registro para no ser eliminado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n",
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test2.show()\n",
    "test2.na.drop(thresh=2).show() # como tiene dos campos con valores infromados aunq el registro tenga nulls no se borra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*subset*  borraremos el registro siempre y cuando aparezca un null en la columna q le pasemos al `subset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n",
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "|     null| 34|        10| 38000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test2.show()\n",
    "test2.na.drop(subset=['Experience']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para sustituir los valores `null` con otro valor usamos la función `fill(value=\"new_value\", subset=[column_name])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "|   Mahesh|  0|         0| 40000|\n",
      "|  missing| 34|        10| 38000|\n",
      "|  missing| 36|         0|     0|\n",
      "|  missing|  0|         0|     0|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = test2.na.fill('missing')\n",
    "test = test.na.fill(0)\n",
    "test.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a05f93782d31fb45d30263a0389582a01d7e14abf3ec6aacde92652303ee35ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
